{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnV5re322Wz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc34093-31af-4733-ea36-d8730bb70026"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt update\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "openjdk-8-jdk-headless is already the newest version (8u462-ga~us1-0ubuntu2~22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhYIAjti3iaf"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_chOpi85JR-O"
      },
      "source": [
        "# Creamos el Spark Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M62onPeJR-O"
      },
      "source": [
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "NuBGkcmqJR-O",
        "outputId": "fc9eae9c-a810-4073-cba5-791269412134"
      },
      "source": [
        "type(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.context.SparkContext"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.context.SparkContext</b><br/>def __init__(master: Optional[str]=None, appName: Optional[str]=None, sparkHome: Optional[str]=None, pyFiles: Optional[List[str]]=None, environment: Optional[Dict[str, Any]]=None, batchSize: int=0, serializer: &#x27;Serializer&#x27;=CPickleSerializer(), conf: Optional[SparkConf]=None, gateway: Optional[JavaGateway]=None, jsc: Optional[JavaObject]=None, profiler_cls: Type[BasicProfiler]=BasicProfiler, udf_profiler_cls: Type[UDFBasicProfiler]=UDFBasicProfiler, memory_profiler_cls: Type[MemoryProfiler]=MemoryProfiler)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/context.py</a>Main entry point for Spark functionality. A SparkContext represents the\n",
              "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
              "broadcast variables on that cluster.\n",
              "\n",
              "When you create a new SparkContext, at least the master and app name should\n",
              "be set, either through the named parameters here or through `conf`.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "master : str, optional\n",
              "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
              "appName : str, optional\n",
              "    A name for your job, to display on the cluster web UI.\n",
              "sparkHome : str, optional\n",
              "    Location where Spark is installed on cluster nodes.\n",
              "pyFiles : list, optional\n",
              "    Collection of .zip or .py files to send to the cluster\n",
              "    and add to PYTHONPATH.  These can be paths on the local file\n",
              "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
              "environment : dict, optional\n",
              "    A dictionary of environment variables to set on\n",
              "    worker nodes.\n",
              "batchSize : int, optional, default 0\n",
              "    The number of Python objects represented as a single\n",
              "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
              "    the batch size based on object sizes, or -1 to use an unlimited\n",
              "    batch size\n",
              "serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
              "    The serializer for RDDs.\n",
              "conf : :class:`SparkConf`, optional\n",
              "    An object setting Spark properties.\n",
              "gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
              "    Use an existing gateway and JVM, otherwise a new JVM\n",
              "    will be instantiated. This is only used internally.\n",
              "jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
              "    The JavaSparkContext instance. This is only used internally.\n",
              "profiler_cls : type, optional, default :class:`BasicProfiler`\n",
              "    A class of custom Profiler used to do profiling\n",
              "udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
              "    A class of custom Profiler used to do udf profiling\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
              "the active :class:`SparkContext` before creating a new one.\n",
              "\n",
              ":class:`SparkContext` instance is not supported to share across multiple\n",
              "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
              "Use threads instead for concurrent processing purpose.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.context import SparkContext\n",
              "&gt;&gt;&gt; sc = SparkContext(&#x27;local&#x27;, &#x27;test&#x27;)\n",
              "&gt;&gt;&gt; sc2 = SparkContext(&#x27;local&#x27;, &#x27;test2&#x27;) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "ValueError: ...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 92);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b5a574a",
        "outputId": "2351168f-0bf1-4f8a-daca-65a175aef5f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ot8-hDKZqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3ea99e-254e-46c6-d6e2-8f1169121f6e"
      },
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/CienciaDeDatos/TP1/data/orders.csv\"\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "df = sqlContext.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
        "\n",
        "selected_columns_df = df.select(\"shipping_address\", \"discount_amount\", \"total_amount\", \"currency\")\n",
        "rdd = selected_columns_df.rdd\n",
        "\n",
        "rdd = rdd.filter(lambda row: row[\"shipping_address\"] is not None and row[\"discount_amount\"] is not None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ90e1PrM6xD",
        "outputId": "0569d23e-9d79-4f37-cbe2-abbaec9ade54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3755451"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Cuál es el estado que más descuentos tiene en total? y en promedio? Supongan que de una direccion del estilo: 3123 Alan Extension Port Andrea, MA 26926, “MA” es el estado.\n",
        "\n",
        "### Hipótesis tomadas:\n",
        "\n",
        "Se consideró al estado como una unidad representada por las 2 letras en el anteúltimo espacio del shipping address, cómo por ejemplo “AP” en “USNV Morrison FPO AP 90901”."
      ],
      "metadata": {
        "id": "o7F-w9jVaHJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cotizaciones\n",
        "rates = {\n",
        "    \"USD\": 1.0,\n",
        "    \"GBP\": 0.7391,\n",
        "    \"CAD\": 1.3869,\n",
        "    \"EUR\": 0.8547\n",
        "}\n",
        "\n",
        "# Funciones de limpieza y normalización:\n",
        "def to_float(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def to_usd(amount, currency):\n",
        "    current_currency = \"USD\" if currency is None else str(currency).strip().upper()\n",
        "    rate = rates.get(current_currency, 1.0)\n",
        "    return amount / rate\n",
        "\n",
        "def get_state(address):\n",
        "    try:\n",
        "        state = str(address.split()[-2].strip().upper())\n",
        "        return state\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def discount_value_to_usd(row):\n",
        "    percentage = to_float(row[\"discount_amount\"])\n",
        "    total_amount = to_float(row[\"total_amount\"])\n",
        "    local_value = (percentage / 100.0) * total_amount\n",
        "    try:\n",
        "        currency = row[\"currency\"]\n",
        "    except Exception:\n",
        "        currency = \"USD\"\n",
        "    return to_usd(local_value, currency)\n"
      ],
      "metadata": {
        "id": "Iy-yLSIB9O9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discounted_orders_usd = (\n",
        "    rdd\n",
        "    .map(lambda row: (get_state(row[\"shipping_address\"]), discount_value_to_usd(row)))\n",
        "    .filter(lambda x: x[0] is not None and x[1] > 0.0)\n",
        ")\n",
        "\n",
        "discount_sum_by_state = discounted_orders_usd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "top_sum_state = discount_sum_by_state.takeOrdered(1, key=lambda x: -x[1])\n",
        "print(\"Top suma descuento (USD):\", top_sum_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG07zxWi9Zva",
        "outputId": "f6d54176-07da-4af2-ed9f-a7fea2244068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top suma descuento (USD): [('AP', 361103.07038235094)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_count_by_state = (\n",
        "    discounted_orders_usd\n",
        "    .mapValues(lambda v: (v, 1))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        ")\n",
        "\n",
        "avg_discount_by_state = sum_count_by_state.mapValues(lambda sc: sc[0] / sc[1])\n",
        "\n",
        "top_avg_state = avg_discount_by_state.takeOrdered(1, key=lambda x: -x[1])\n",
        "print(\"Top promedio descuento (USD):\", top_avg_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMPQg8h4_CU8",
        "outputId": "2a3e4ebe-9d67-4753-f237-220a90e293bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top promedio descuento (USD): [('AP', 13.19146161987108)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones:\n",
        "\n",
        "El resultado mostró a AP como el estado con mayor monto descontado, en total y en promedio, con USD 361103.07 en total y USD 13.26 por orden.\n",
        "Respecto al procesamiento realizado sin Spark, se notó que este contenía un mayor monto descontado total, de 348050.53. Esto puede deberse a que en esta consulta fuimos menos estrictos con los filtros de valores nulos. El promedio de descuento por estado fue exactamente el mismo valor, así que se valida que el procesamiento se dió exitosamente."
      ],
      "metadata": {
        "id": "mZP4Ms1aGKKM"
      }
    }
  ]
}