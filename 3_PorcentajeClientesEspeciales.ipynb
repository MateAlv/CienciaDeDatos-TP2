{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnV5re322Wz9",
        "outputId": "685df6f9-430e-4928-cda4-8ecd5f641517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,065 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,809 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,323 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,275 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,415 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,744 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,922 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Fetched 30.5 MB in 4s (7,674 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u462-ga~us1-0ubuntu2~22.04.2_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u462-ga~us1-0ubuntu2~22.04.2_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt update\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nhYIAjti3iaf"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_chOpi85JR-O"
      },
      "source": [
        "# Creamos el Spark Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3M62onPeJR-O"
      },
      "outputs": [],
      "source": [
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "NuBGkcmqJR-O",
        "outputId": "71780752-f471-42a5-f6cf-ac53ad7b1fa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.context.SparkContext"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.context.SparkContext</b><br/>def __init__(master: Optional[str]=None, appName: Optional[str]=None, sparkHome: Optional[str]=None, pyFiles: Optional[List[str]]=None, environment: Optional[Dict[str, Any]]=None, batchSize: int=0, serializer: &#x27;Serializer&#x27;=CPickleSerializer(), conf: Optional[SparkConf]=None, gateway: Optional[JavaGateway]=None, jsc: Optional[JavaObject]=None, profiler_cls: Type[BasicProfiler]=BasicProfiler, udf_profiler_cls: Type[UDFBasicProfiler]=UDFBasicProfiler, memory_profiler_cls: Type[MemoryProfiler]=MemoryProfiler)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/context.py</a>Main entry point for Spark functionality. A SparkContext represents the\n",
              "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
              "broadcast variables on that cluster.\n",
              "\n",
              "When you create a new SparkContext, at least the master and app name should\n",
              "be set, either through the named parameters here or through `conf`.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "master : str, optional\n",
              "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
              "appName : str, optional\n",
              "    A name for your job, to display on the cluster web UI.\n",
              "sparkHome : str, optional\n",
              "    Location where Spark is installed on cluster nodes.\n",
              "pyFiles : list, optional\n",
              "    Collection of .zip or .py files to send to the cluster\n",
              "    and add to PYTHONPATH.  These can be paths on the local file\n",
              "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
              "environment : dict, optional\n",
              "    A dictionary of environment variables to set on\n",
              "    worker nodes.\n",
              "batchSize : int, optional, default 0\n",
              "    The number of Python objects represented as a single\n",
              "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
              "    the batch size based on object sizes, or -1 to use an unlimited\n",
              "    batch size\n",
              "serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
              "    The serializer for RDDs.\n",
              "conf : :class:`SparkConf`, optional\n",
              "    An object setting Spark properties.\n",
              "gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
              "    Use an existing gateway and JVM, otherwise a new JVM\n",
              "    will be instantiated. This is only used internally.\n",
              "jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
              "    The JavaSparkContext instance. This is only used internally.\n",
              "profiler_cls : type, optional, default :class:`BasicProfiler`\n",
              "    A class of custom Profiler used to do profiling\n",
              "udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
              "    A class of custom Profiler used to do udf profiling\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
              "the active :class:`SparkContext` before creating a new one.\n",
              "\n",
              ":class:`SparkContext` instance is not supported to share across multiple\n",
              "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
              "Use threads instead for concurrent processing purpose.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.context import SparkContext\n",
              "&gt;&gt;&gt; sc = SparkContext(&#x27;local&#x27;, &#x27;test&#x27;)\n",
              "&gt;&gt;&gt; sc2 = SparkContext(&#x27;local&#x27;, &#x27;test2&#x27;) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "ValueError: ...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 92);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "type(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b5a574a",
        "outputId": "af976823-b584-4e82-c7bd-f2024fcb4ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7ot8-hDKZqR",
        "outputId": "461f3736-a9c5-4536-ee8d-945227380aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/CienciaDeDatos/TP1/data/orders.csv\"\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "df = sqlContext.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
        "\n",
        "selected_columns_df = df.select(\"shipping_address\", \"discount_amount\", \"total_amount\", \"currency\")\n",
        "rdd = selected_columns_df.rdd\n",
        "\n",
        "rdd = rdd.filter(lambda row: row[\"shipping_address\"] is not None and row[\"discount_amount\"] is not None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yqE2Ls5q8R2"
      },
      "source": [
        "# 3) Para cada tipo de pago y segmento de cliente, devolver la suma y el promedio expresado como porcentaje, de clientes activos y de consentimiento de marketing. Se valora que el output de la consulta tenga nombres claros y en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeI_JwdEgI_Y",
        "outputId": "bb6fc7c1-422c-4417-cf3e-5f24b6978fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "DATA_PATH = \"/content/drive/MyDrive/CienciaDeDatos/TP1/data/\"\n",
        "\n",
        "rdd_orders = (\n",
        "    sqlContext.read.csv(DATA_PATH + 'orders.csv', header=True, inferSchema=True)\n",
        "    .select(\"payment_method\", \"customer_id\")\n",
        "    .rdd\n",
        ")\n",
        "\n",
        "rdd_customers = (\n",
        "    sqlContext.read.csv(DATA_PATH + 'customers.csv', header=True, inferSchema=True)\n",
        "    .select('customer_id', 'customer_segment','is_active','marketing_consent')\n",
        "    .rdd\n",
        ")\n",
        "\n",
        "TOP_CPS_SHOWED = 5\n",
        "TOP_NAMES_SHOWED = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext, functions as F\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "# ------------------- Helpers -------------------\n",
        "def normalize_category(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    v = str(value).strip().lower()\n",
        "    if v in {\"nan\", \"na\", \"undefined\", \"none\", \"\"}:\n",
        "        return None\n",
        "    return v.title()\n",
        "\n",
        "def to_int_bool(v):\n",
        "    if v is None:\n",
        "        return 0\n",
        "    if isinstance(v, bool):\n",
        "        return 1 if v else 0\n",
        "    s = str(v).strip().lower()\n",
        "    if s in {\"true\", \"1\", \"si\", \"sí\", \"y\", \"yes\"}:\n",
        "        return 1\n",
        "    if s in {\"false\", \"0\", \"no\", \"n\"}:\n",
        "        return 0\n",
        "    try:\n",
        "        return 1 if int(float(s)) != 0 else 0\n",
        "    except:\n",
        "        return 0\n",
        "# ------------------- Normalizaciones -------------------\n",
        "rdd_orders_nm = (\n",
        "    rdd_orders\n",
        "    .map(lambda r: (r[\"customer_id\"], normalize_category(r[\"payment_method\"])))\n",
        "    .filter(lambda kv: kv[0] is not None and kv[1] is not None)\n",
        "    .distinct()\n",
        ")\n",
        "\n",
        "rdd_customers_nm = (\n",
        "    rdd_customers\n",
        "    .map(lambda r: (\n",
        "        r[\"customer_id\"],\n",
        "        (\n",
        "            normalize_category(r[\"customer_segment\"]),\n",
        "            to_int_bool(r[\"is_active\"]),\n",
        "            to_int_bool(r[\"marketing_consent\"])\n",
        "        )\n",
        "    ))\n",
        "    .filter(lambda kv: kv[0] is not None and kv[1][0] is not None)\n",
        ")\n",
        "\n",
        "# (customer_id, ((segmento, activo, consent), tipo_pago))\n",
        "rdd_join = rdd_customers_nm.join(rdd_orders_nm)\n",
        "\n",
        "# Key: (tipo_pago, segmento)\n",
        "# Value: (suma_activos, suma_consentimiento, total_clientes)\n",
        "rdd_kv = rdd_join.map(lambda item: (\n",
        "    (item[1][1], item[1][0][0]),\n",
        "    (item[1][0][1], item[1][0][2], 1)\n",
        "))\n",
        "\n",
        "rdd_sums = rdd_kv.reduceByKey(lambda a, b: (\n",
        "    a[0] + b[0],   # activos\n",
        "    a[1] + b[1],   # consentimiento\n",
        "    a[2] + b[2]    # total\n",
        "))\n",
        "\n",
        "rdd_result = rdd_sums.map(lambda kv: {\n",
        "    \"tipo_pago\": kv[0][0],\n",
        "    \"segmento\": kv[0][1],\n",
        "    \"clientes_totales\": kv[1][2],\n",
        "    \"clientes_activos_suma\": kv[1][0],\n",
        "    \"clientes_activos_promedio_%\": 100.0 * kv[1][0] / kv[1][2] if kv[1][2] else 0.0,\n",
        "    \"consentimiento_marketing_suma\": kv[1][1],\n",
        "    \"consentimiento_marketing_promedio_%\": 100.0 * kv[1][1] / kv[1][2] if kv[1][2] else 0.0\n",
        "})\n",
        "\n",
        "# ------------------- Pivots -- -------------------\n",
        "df_result = sqlContext.createDataFrame(rdd_result)\n",
        "\n",
        "\n",
        "pivot_activos = (\n",
        "    df_result\n",
        "    .groupBy(\"tipo_pago\")\n",
        "    .pivot(\"segmento\")\n",
        "    .agg(F.first(\"clientes_activos_promedio_%\"))\n",
        "    .orderBy(\"tipo_pago\")\n",
        ")\n",
        "\n",
        "\n",
        "pivot_consent = (\n",
        "    df_result\n",
        "    .groupBy(\"tipo_pago\")\n",
        "    .pivot(\"segmento\")\n",
        "    .agg(F.first(\"consentimiento_marketing_promedio_%\"))\n",
        "    .orderBy(\"tipo_pago\")\n",
        ")\n",
        "\n",
        "print(\"----- Porcentajes de Clientes Activos -----\")\n",
        "pivot_activos.show(truncate=False)\n",
        "\n",
        "print(\"----- Porcentajes de Consentimiento Marketing -----\")\n",
        "pivot_consent.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrQ281IR1g4L",
        "outputId": "45efdec5-0f95-49ba-b63f-5ee0e3ba60b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Porcentajes de Clientes Activos -----\n",
            "+----------------+-----------------+-----------------+-----------------+\n",
            "|tipo_pago       |Budget           |Premium          |Regular          |\n",
            "+----------------+-----------------+-----------------+-----------------+\n",
            "|Bank Transfer   |89.78004141713774|89.88082221736278|89.96447276856023|\n",
            "|Cash On Delivery|89.79454738845659|89.87860892388451|89.96704503844745|\n",
            "|Credit Card     |89.77889728519452|89.87203324948048|89.96228073387776|\n",
            "|Debit Card      |89.77603583426652|89.88628908812596|89.97179797091894|\n",
            "|Digital Wallet  |89.78894922465432|89.88518316019683|89.96630406914991|\n",
            "|Paypal          |89.78894922465432|89.87750191403259|89.96338337605273|\n",
            "+----------------+-----------------+-----------------+-----------------+\n",
            "\n",
            "----- Porcentajes de Consentimiento Marketing -----\n",
            "+----------------+-----------------+-----------------+-----------------+\n",
            "|tipo_pago       |Budget           |Premium          |Regular          |\n",
            "+----------------+-----------------+-----------------+-----------------+\n",
            "|Bank Transfer   |69.68713270274809|70.01968073474742|70.13331868292862|\n",
            "|Cash On Delivery|69.69154117449476|70.02952755905511|70.13365067740754|\n",
            "|Credit Card     |69.67814161768821|70.03718691895439|70.13036950232541|\n",
            "|Debit Card      |69.65845464725643|70.03061447627378|70.12965608174926|\n",
            "|Digital Wallet  |69.66914851928567|70.02733734281028|70.13698128410797|\n",
            "|Paypal          |69.67474668308795|70.0426555835065 |70.13181984621018|\n",
            "+----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones:\n",
        "\n",
        "Se observó una sorpresivamente baja variabilidad de porcentajes entre métodos y segmentos: la tasa de clientes activos se ubica en torno al 89,8–90,0% para todos los clientes y la de consentimiento de marketing alrededor del 69,7–70,1%.\n",
        "A partir de eso se concluye que el segmento de cliente y el método de pago de elección a la hora de realizar una orden, no tienen correlación con la probabilidad de que el cliente sea activo o que consienta al acuerdo de marketing.\n",
        "Fuera de esto, sí se observó una sola distinción entre los valores y es que hay muchos más clientes en el segmento Regular que en los otros segmentos. Se puede pensar que esto es debido a que el servicio probablemente sea pago."
      ],
      "metadata": {
        "id": "12b__iHgHZLZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}