{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnV5re322Wz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89c42dd-bba9-4a33-b1f7-a6264ad68655"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt update\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,275 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,065 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,415 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,323 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,809 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,738 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 24.7 MB in 3s (8,083 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u462-ga~us1-0ubuntu2~22.04.2_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u462-ga~us1-0ubuntu2~22.04.2_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u462-ga~us1-0ubuntu2~22.04.2) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhYIAjti3iaf"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_chOpi85JR-O"
      },
      "source": [
        "# Creamos el Spark Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M62onPeJR-O"
      },
      "source": [
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "NuBGkcmqJR-O",
        "outputId": "baee6b25-a0be-4cad-ff10-ef628da788d1"
      },
      "source": [
        "type(sc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.context.SparkContext"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.context.SparkContext</b><br/>def __init__(master: Optional[str]=None, appName: Optional[str]=None, sparkHome: Optional[str]=None, pyFiles: Optional[List[str]]=None, environment: Optional[Dict[str, Any]]=None, batchSize: int=0, serializer: &#x27;Serializer&#x27;=CPickleSerializer(), conf: Optional[SparkConf]=None, gateway: Optional[JavaGateway]=None, jsc: Optional[JavaObject]=None, profiler_cls: Type[BasicProfiler]=BasicProfiler, udf_profiler_cls: Type[UDFBasicProfiler]=UDFBasicProfiler, memory_profiler_cls: Type[MemoryProfiler]=MemoryProfiler)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/context.py</a>Main entry point for Spark functionality. A SparkContext represents the\n",
              "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
              "broadcast variables on that cluster.\n",
              "\n",
              "When you create a new SparkContext, at least the master and app name should\n",
              "be set, either through the named parameters here or through `conf`.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "master : str, optional\n",
              "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
              "appName : str, optional\n",
              "    A name for your job, to display on the cluster web UI.\n",
              "sparkHome : str, optional\n",
              "    Location where Spark is installed on cluster nodes.\n",
              "pyFiles : list, optional\n",
              "    Collection of .zip or .py files to send to the cluster\n",
              "    and add to PYTHONPATH.  These can be paths on the local file\n",
              "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
              "environment : dict, optional\n",
              "    A dictionary of environment variables to set on\n",
              "    worker nodes.\n",
              "batchSize : int, optional, default 0\n",
              "    The number of Python objects represented as a single\n",
              "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
              "    the batch size based on object sizes, or -1 to use an unlimited\n",
              "    batch size\n",
              "serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
              "    The serializer for RDDs.\n",
              "conf : :class:`SparkConf`, optional\n",
              "    An object setting Spark properties.\n",
              "gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
              "    Use an existing gateway and JVM, otherwise a new JVM\n",
              "    will be instantiated. This is only used internally.\n",
              "jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
              "    The JavaSparkContext instance. This is only used internally.\n",
              "profiler_cls : type, optional, default :class:`BasicProfiler`\n",
              "    A class of custom Profiler used to do profiling\n",
              "udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
              "    A class of custom Profiler used to do udf profiling\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
              "the active :class:`SparkContext` before creating a new one.\n",
              "\n",
              ":class:`SparkContext` instance is not supported to share across multiple\n",
              "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
              "Use threads instead for concurrent processing purpose.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.context import SparkContext\n",
              "&gt;&gt;&gt; sc = SparkContext(&#x27;local&#x27;, &#x27;test&#x27;)\n",
              "&gt;&gt;&gt; sc2 = SparkContext(&#x27;local&#x27;, &#x27;test2&#x27;) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "ValueError: ...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 92);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b5a574a",
        "outputId": "f3afcb18-9d72-4f32-d757-d8151a2d362c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Para los productos que contienen en su descripción la palabra “stuff” (sin importar mayúsculas o minúsculas), calcular el peso total de su inventario agrupado por marca, mostrar sólo la marca y el peso total de las 5 más pesadas."
      ],
      "metadata": {
        "id": "231s7NixH1c6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ot8-hDKZqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e360bd3c-ed48-478d-b65a-95f286c9490a"
      },
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/CienciaDeDatos/TP1/data/\"\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "rdd_products = (\n",
        "    sqlContext.read.csv(DATA_PATH + \"products.csv\", header=True, inferSchema=True)\n",
        "    .select(\"product_id\", \"description\", 'weight_kg', 'brand')\n",
        "    .rdd\n",
        "    )\n",
        "\n",
        "rdd_inventory_logs = (\n",
        "    sqlContext.read.csv(DATA_PATH + 'inventory_logs.csv', header=True, inferSchema=True)\n",
        "    .select('product_id', 'quantity_change')\n",
        "    .rdd\n",
        ")\n",
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DESCRIPTION_KEYWORD = \"stuff\"\n",
        "BRAND_AMOUNT_TO_SHOW = 5\n",
        "\n",
        "def normalize_category(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    v = str(value).strip().lower()\n",
        "    if v in {\"nan\", \"na\", \"undefined\", \"none\", \"\"}:\n",
        "        return None\n",
        "    return v.title()\n",
        "\n",
        "rdd_products_nm = (\n",
        "    rdd_products\n",
        "    .filter(lambda row: row[\"weight_kg\"] is not None)\n",
        "    .map(lambda row: (int(row[\"product_id\"]),\n",
        "        (\n",
        "          normalize_category(row[\"description\"]),\n",
        "          float(row[\"weight_kg\"]),\n",
        "          normalize_category(row[\"brand\"])\n",
        "        )\n",
        "    ))\n",
        "    .filter(lambda row:\n",
        "        row[1][0] and row[1][1] is not None and row[1][2] is not None and\n",
        "        DESCRIPTION_KEYWORD.lower() in row[1][0].lower()\n",
        "    )\n",
        ")\n",
        "\n",
        "rdd_inventory_amount = (\n",
        "    rdd_inventory_logs\n",
        "    .filter(lambda row: row[\"quantity_change\"] is not None)\n",
        "    .map(lambda row: (int(row[\"product_id\"]), int(row[\"quantity_change\"])))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "rdd_products_inventory = rdd_products_nm.join(rdd_inventory_amount)\n",
        "\n",
        "weight_count_per_brand = (\n",
        "    rdd_products_inventory\n",
        "    .map( lambda row: (row[1][0][2], (row[1][0][1] * row[1][1])) )\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "top_brands = weight_count_per_brand.takeOrdered(BRAND_AMOUNT_TO_SHOW, key=lambda x: -x[1])\n",
        "\n",
        "print(top_brands)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n89HI7252ifM",
        "outputId": "b37ca2d2-5dc4-4c60-9e84-36205a4796bd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Stubhub', 92855.6), ('Cricut', 88683.57), ('Bosch', 70196.71), ('Pilot', 67907.37), ('Universal Music', 64708.659999999996)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones:\n",
        "Se obtuvieron las cinco marcas con mayor peso total:\n",
        " - Stubhub: 92855.60 kg\n",
        " - Cricut:  88683.57 kg\n",
        " - Bosch: 70196.71 kg\n",
        " - Pilot:  67907.37 kg\n",
        " - Universal Music: 64708.66 kg\n",
        "\n",
        "Lamentablemente, esta vez se observaron notables diferencias con el procesamiento anterior hecho solo sin SPARK. Los resultados muestran valores distintos para los pesos sumados de cada marca. Se intentó extensivamente encontrar la manera de matchear los resultados, sin éxito. Es probable que se deba al manejo distinto de valores inválidos y nulos. Se muestran números similares, incluso para algunas marcas el valor es el mismo, pero en general es un resultado distinto y esto es claramente un error en el código de una de las dos consultas.\n",
        "\n",
        "### Resultados sin SPARK:\n",
        " - Stubhub: 92855.60 kg\n",
        " - Bosch: 92757.51 kg\n",
        " - Cricut:  82866.07 kg\n",
        " - Sony Music: 81667.15 kg\n",
        " - Pilot:  67907.37 kg\n"
      ],
      "metadata": {
        "id": "HxJystyJMZC7"
      }
    }
  ]
}